{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6fdbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5629358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('intents.json') as file :\n",
    "file = open(f'intents.json', encoding = 'UTF-8')\n",
    "data = json.load(file)\n",
    "    \n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "labels = []\n",
    "responses = []\n",
    "\n",
    "for intent in data['intents'] : \n",
    "    for pattern in intent['patterns'] : \n",
    "        training_sentences.append(pattern)\n",
    "        training_labels.append(intent['tag'])\n",
    "    responses.append(intent['responses'])\n",
    "    \n",
    "    if intent['tag'] not in labels :\n",
    "        labels.append(intent['tag'])\n",
    "\n",
    "num_classes = len(labels)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc37ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_encoder = LabelEncoder()\n",
    "lbl_encoder.fit(training_labels)\n",
    "training_labels = lbl_encoder.transform(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb7d96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "embedding_dim = 16\n",
    "max_len = 20\n",
    "oov_token = '<OOV>'\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_token)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bb0667",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f7dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b321a9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chat_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('chat_model')\n",
    "\n",
    "import pickle\n",
    "\n",
    "#save\n",
    "with open('tokenizer.pickle', 'wb') as handle : \n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#save\n",
    "with open('label_encoder.pickle', 'wb') as ecn_file : \n",
    "    pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorama\n",
    "colorama.init()\n",
    "from colorama import Fore, Style, Back\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "with open('intents.json') as file : \n",
    "    data = json.load(file)\n",
    "\n",
    "def chat() : \n",
    "    # load trained model\n",
    "    model = keras.models.load_model('chat_model')\n",
    "    \n",
    "    # load tokenizer object\n",
    "    with open('tokenizer.pickle', 'rb') as handle : \n",
    "        tokenizer = pickle.load(handle)\n",
    "    \n",
    "    with open('label_encoder.pickle', 'rb') as enc : \n",
    "        lbl_encoder = pickle.load(enc)\n",
    "    \n",
    "    # parameters\n",
    "    max_len = 20\n",
    "    \n",
    "    while True : \n",
    "        print(Fore.LIGHTBLUE_EX + 'User : ' + Style.RESET_ALL, end = '')\n",
    "        inp = input()\n",
    "        if inp.lower() == 'quit' : \n",
    "            break\n",
    "        \n",
    "        result = model.predict(keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([inp]), truncating='post', maxlen=max_len))\n",
    "        \n",
    "        tag = lbl_encoder.inverse_transform([np.argmax(result)])\n",
    "        \n",
    "        for i in data['intents'] : \n",
    "            if i['tag'] == tag :\n",
    "                print(Fore.GREEN + 'ChatBot: ' + Style.RESET_ALL, np.random.choice(i['responses']))\n",
    "\n",
    "\n",
    "                \n",
    "print(Fore.YELLOW + 'Start messaging with the bot (type quit to stop)! ' + Style.RESET_ALL)\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b8849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
